{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "enriching_fasttext.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6JE9VAcLY2gi2i0gtgmZP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9urcfcw42KNj"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook we will try to ablate on some of the discrepancies that we noticed with the [tensorflow official tutorial for Word2Vec](https://www.tensorflow.org/tutorials/text/word2vec).\n",
        "\n",
        "To be precise we are looking forward to ablate on two pointers:\n",
        "- Using the text vectorization layer\n",
        "- Customizing the negative pairs so that we do not have negative words from the window specified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_tJfAiQ2HF1",
        "outputId": "4920c3c6-4d3b-40e7-bd8e-ee7d75868fe2"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "SEED = 42 \n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SELYYmUt3GCw"
      },
      "source": [
        "# Data\n",
        "We will be working on the same data that the official guide uses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DJwhEkl3eCH"
      },
      "source": [
        "from tensorflow.keras.utils import get_file"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8j_en4D3FpA",
        "outputId": "69df1138-7c7a-4db9-a0b1-44c5ce147d60"
      },
      "source": [
        "# Shakespear text file\n",
        "path_to_file = tf.keras.utils.get_file(fname='shakespeare.txt',\n",
        "                                       origin='https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "print(f'[INFO] Path to file: {path_to_file}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "[INFO] Path to file: /root/.keras/datasets/shakespeare.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5RmonRW364e"
      },
      "source": [
        "## Text\n",
        "Here in this snippet we will look into the text file. I would suggest people to take some time out and look into the data, even if it is just glancing it once. This step is not mandatory, but does build a mental map of what we are going to model up on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTfzNhM13DT2",
        "outputId": "2d78eec7-e51c-4c31-83ed-e250e7248043"
      },
      "source": [
        "# To vizualise the text data\n",
        "with open(path_to_file) as f:\n",
        "    lines = f.read().splitlines()\n",
        "for line in lines[:5]:\n",
        "    print(line)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zypPcoTu4a3T",
        "outputId": "6b23cd2c-7270-48b8-fa6b-6150bd54a361"
      },
      "source": [
        "# Create a `tf.data` with all the non-negative sentences\n",
        "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
        "\n",
        "for text in text_ds.take(5):\n",
        "    print(text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'First Citizen:', shape=(), dtype=string)\n",
            "tf.Tensor(b'Before we proceed any further, hear me speak.', shape=(), dtype=string)\n",
            "tf.Tensor(b'All:', shape=(), dtype=string)\n",
            "tf.Tensor(b'Speak, speak.', shape=(), dtype=string)\n",
            "tf.Tensor(b'First Citizen:', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SA3sQo1QhO1"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import re\n",
        "import string"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wvmrzbG4zNZ"
      },
      "source": [
        "# We create a custom standardization function to lowercase the text and \n",
        "# remove punctuation.\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    return tf.strings.regex_replace(lowercase,\n",
        "                                    '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "# Define the vocabulary size and number of words in a sequence.\n",
        "vocab_size = 4096\n",
        "sequence_length = 10\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Set output_sequence_length length to pad all samples to same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "# build the vocab\n",
        "vectorize_layer.adapt(text_ds.batch(1024))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO6QpTbTQcFs",
        "outputId": "1b6e705e-b83f-4964-9762-e3ccf3cbfa19"
      },
      "source": [
        "# Save the created vocabulary for reference.\n",
        "index_word = vectorize_layer.get_vocabulary()\n",
        "print(index_word[:10])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_ZUiufoGYxM"
      },
      "source": [
        "# Build the sub words\n",
        "What we have:\n",
        "- `index_word`: A list of all the unique words in the vocab\n",
        "\n",
        "What we want:\n",
        "- `subword_index`: A dictionary that maps subwords to its unique index\n",
        "- `index_subword`: A dictionary that maps indices to the subword\n",
        "\n",
        "\n",
        "- `word_subwords`: A dictionary that maps a word with all the possible subwords that is has"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP-9FDpVGYWi"
      },
      "source": [
        "subword_index= {}\n",
        "index = 0\n",
        "\n",
        "word_subwords = {}\n",
        "\n",
        "for idx,word in enumerate(index_word):\n",
        "    word = f\"<{word}>\"\n",
        "    if len(word) > 3 and word != \"<[UNK]>\":\n",
        "        dummylist = [word[i:i+3] for i in range(len(word)-2)]\n",
        "        dummylist.append(word)\n",
        "        ind_list = []\n",
        "        for w in dummylist:\n",
        "            if w in subword_index:\n",
        "                ind_list.append(subword_index[w])\n",
        "            else:\n",
        "                index += 1\n",
        "                subword_index[w] = index\n",
        "                ind_list.append(index)\n",
        "        word_subwords[idx] = ind_list\n",
        "    else:\n",
        "        ind_list = []\n",
        "        if word in subword_index:\n",
        "            ind_list.append(subword_index[word])\n",
        "        else:\n",
        "            index += 1\n",
        "            subword_index[word] = index\n",
        "            ind_list.append(index)\n",
        "        word_subwords[idx] = ind_list\n",
        "\n",
        "index_subword = {index:subword for subword,index in subword_index.items()}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5xGIpP8QP7c",
        "outputId": "14360628-5af7-4deb-c92c-447a6ccaec0e"
      },
      "source": [
        "index = 3890\n",
        "\n",
        "# Bridge the gap\n",
        "word = index_word[index]\n",
        "print(f'The word is: {word}')\n",
        "\n",
        "subwords = word_subwords[index]\n",
        "print(f'The subword indices: {subwords}')\n",
        "\n",
        "print('The subwords:', end=' ')\n",
        "for s in subwords:\n",
        "    print(f\"'{index_subword[s]}'\", end=' ')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The word is: whereer\n",
            "The subword indices: [89, 234, 119, 201, 1302, 1404, 120, 6826]\n",
            "The subwords: '<wh' 'whe' 'her' 'ere' 'ree' 'eer' 'er>' '<whereer>' "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX36CAq2Ruxd"
      },
      "source": [
        "subword_vocab_size = len(subword_index)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9akWSlstMz_T",
        "outputId": "dd4ccd03-aa7a-4b4d-a604-7590f40b56cf"
      },
      "source": [
        "print(f'Number of unique words: {vocab_size}')\n",
        "print(f'Number of unique subwords: {subword_vocab_size}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique words: 4096\n",
            "Number of unique subwords: 7114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GGnYjuvRNEk"
      },
      "source": [
        "# Vectorize the data in text_ds.\n",
        "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afNzzNrrRVhK",
        "outputId": "765fe53f-6ec1-43db-cd84-f7d388155240"
      },
      "source": [
        "for text in text_vector_ds.take(2):\n",
        "    print(text)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([ 89 270   0   0   0   0   0   0   0   0], shape=(10,), dtype=int64)\n",
            "tf.Tensor([138  36 982 144 673 125  16 106   0   0], shape=(10,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0-VvNYlRX4J",
        "outputId": "1d9735c5-ba8a-40c7-e361-775129a8909f"
      },
      "source": [
        "# sequences is a list of numpy arrays\n",
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(len(sequences))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJG_hecvRb7E",
        "outputId": "b52ea7fb-b51a-4f8b-a70a-c4a44fe85444"
      },
      "source": [
        "for seq in sequences[99:102]:\n",
        "  print(f\"{seq} => {[index_word[i] for i in seq]}\")\n",
        "  print(f\"{seq} => {[[index_subword[a] for a in word_subwords[i]] for i in seq]}\")\n",
        "  print()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2336 2883 1830    4    1  111    3    1    0    0] => ['piercing', 'statutes', 'daily', 'to', '[UNK]', 'up', 'and', '[UNK]', '', '']\n",
            "[2336 2883 1830    4    1  111    3    1    0    0] => [['<pi', 'pie', 'ier', 'erc', 'rci', 'cin', 'ing', 'ng>', '<piercing>'], ['<st', 'sta', 'tat', 'atu', 'tut', 'ute', 'tes', 'es>', '<statutes>'], ['<da', 'dai', 'ail', 'ily', 'ly>', '<daily>'], ['<to', 'to>', '<to>'], ['<[UNK]>'], ['<up', 'up>', '<up>'], ['<an', 'and', 'nd>', '<and>'], ['<[UNK]>'], ['<>'], ['<>']]\n",
            "\n",
            "[   2  172   39    2  664 1126   79   13  111   60] => ['the', 'poor', 'if', 'the', 'wars', 'eat', 'us', 'not', 'up', 'they']\n",
            "[   2  172   39    2  664 1126   79   13  111   60] => [['<th', 'the', 'he>', '<the>'], ['<po', 'poo', 'oor', 'or>', '<poor>'], ['<if', 'if>', '<if>'], ['<th', 'the', 'he>', '<the>'], ['<wa', 'war', 'ars', 'rs>', '<wars>'], ['<ea', 'eat', 'at>', '<eat>'], ['<us', 'us>', '<us>'], ['<no', 'not', 'ot>', '<not>'], ['<up', 'up>', '<up>'], ['<th', 'the', 'hey', 'ey>', '<they>']]\n",
            "\n",
            "[413  34   2  77  60 203  79   0   0   0] => ['theres', 'all', 'the', 'love', 'they', 'bear', 'us', '', '', '']\n",
            "[413  34   2  77  60 203  79   0   0   0] => [['<th', 'the', 'her', 'ere', 'res', 'es>', '<theres>'], ['<al', 'all', 'll>', '<all>'], ['<th', 'the', 'he>', '<the>'], ['<lo', 'lov', 'ove', 've>', '<love>'], ['<th', 'the', 'hey', 'ey>', '<they>'], ['<be', 'bea', 'ear', 'ar>', '<bear>'], ['<us', 'us>', '<us>'], ['<>'], ['<>'], ['<>']]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyeTHjqERvXn"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxM17D0dfeUF",
        "outputId": "72c68b1e-66f4-46c5-f5b3-dd20d8fee672"
      },
      "source": [
        "sequence = sequences[250]\n",
        "print(\"The Sequence:\")\n",
        "print(f\"{sequence} => {[index_word[i] for i in sequence]}\")\n",
        "\n",
        "list_of_words = list(range(vocab_size))\n",
        "window_size = 4\n",
        "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "# Generate positive skip-gram pairs for a sequence (sentence).\n",
        "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "    sequence, \n",
        "    vocabulary_size=vocab_size,\n",
        "    sampling_table=sampling_table,\n",
        "    window_size=window_size,\n",
        "    negative_samples=0)\n",
        "\n",
        "print(\"Positive Skip Grams:\")\n",
        "print(positive_skip_grams)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Sequence:\n",
            "[  15 2853    6  104    1 1760   25  467    0    0] => ['with', 'thousands', 'of', 'these', '[UNK]', 'slaves', 'as', 'high', '', '']\n",
            "Positive Skip Grams:\n",
            "[[2853, 15], [2853, 104], [2853, 6], [2853, 1760], [2853, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXwOK-WeZMQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3657534f-33c0-480c-f00e-7a179c5929ff"
      },
      "source": [
        "num_ns = 4\n",
        "# Iterate over each positive skip-gram pair to produce training examples \n",
        "# with positive context word and negative samples.\n",
        "for target_word, context_word in positive_skip_grams:\n",
        "    print(f\"Target : {target_word}\")\n",
        "    print(f\"Target Word: {index_word[target_word]}\")\n",
        "\n",
        "    context_words = [context if target == target_word else -1 for target,context in positive_skip_grams] + [target_word]\n",
        "    context_words = list(filter(lambda x: x != -1, context_words))\n",
        "\n",
        "    context_class = tf.expand_dims(\n",
        "        tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "    \n",
        "    negative_words = list(filter(lambda i: i not in context_words, list_of_words))\n",
        "    negative_sampling_candidates = tf.constant(random.sample(negative_words, num_ns), dtype=\"int64\")\n",
        "\n",
        "    # Build context and label vectors (for one target word)\n",
        "    negative_sampling_candidates = tf.expand_dims(\n",
        "        negative_sampling_candidates, 1)\n",
        "\n",
        "    context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "    print(f\"Context: {context}\")\n",
        "    print(f\"Context Shape: {context.shape}\")\n",
        "    label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "    print(f\"Label: {label}\")\n",
        "    print(f\"Label Shape: {label.shape}\")\n",
        "    \n",
        "    # Append each element from the training example to global lists.\n",
        "    subwords = word_subwords[target_word]\n",
        "    \n",
        "    print(f\"Subwords: {subwords}\")\n",
        "    print(f\"{[index_subword[word] for word in subwords]}\")\n",
        "    # targets.append(sub_tar)\n",
        "    # contexts.append(context)\n",
        "    # labels.append(label)\n",
        "    \n",
        "    break"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target : 2853\n",
            "Target Word: thousands\n",
            "Context: [[  15]\n",
            " [ 753]\n",
            " [3257]\n",
            " [ 219]\n",
            " [ 951]]\n",
            "Context Shape: (5, 1)\n",
            "Label: [1 0 0 0 0]\n",
            "Label Shape: (5,)\n",
            "Subwords: [3, 80, 81, 766, 1153, 1154, 8, 710, 711, 5463]\n",
            "['<th', 'tho', 'hou', 'ous', 'usa', 'san', 'and', 'nds', 'ds>', '<thousands>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-kfMd1gSIgP"
      },
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "    # Elements of each training example are appended to these lists.\n",
        "    targets, contexts, labels = [], [], []\n",
        "    \n",
        "    # will be used to sample\n",
        "    list_of_words = list(range(vocab_size))\n",
        "    \n",
        "    # Build the sampling table for vocab_size tokens.\n",
        "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "    \n",
        "    # Iterate over all sequences (sentences) in dataset.\n",
        "    for sequence in tqdm(sequences):\n",
        "        # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "            sequence, \n",
        "            vocabulary_size=vocab_size,\n",
        "            sampling_table=sampling_table,\n",
        "            window_size=window_size,\n",
        "            negative_samples=0)\n",
        "        \n",
        "        # Iterate over each positive skip-gram pair to produce training examples \n",
        "        # with positive context word and negative samples.\n",
        "        for target_word, context_word in positive_skip_grams:\n",
        "            context_words = [context if target == target_word else -1 for target,context in positive_skip_grams] + [target_word]\n",
        "            context_words = list(filter(lambda x: x != -1, context_words))\n",
        "\n",
        "            context_class = tf.expand_dims(\n",
        "                tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "            \n",
        "            negative_words = list(filter(lambda i: i not in context_words, list_of_words))\n",
        "            negative_sampling_candidates = tf.constant(random.sample(negative_words, num_ns), dtype=\"int64\")\n",
        "\n",
        "            # Build context and label vectors (for one target word)\n",
        "            negative_sampling_candidates = tf.expand_dims(\n",
        "                negative_sampling_candidates, 1)\n",
        "\n",
        "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "            \n",
        "            # Append each element from the training example to global lists.\n",
        "            subwords = word_subwords[target_word]\n",
        "            \n",
        "            targets.append(subwords)\n",
        "            contexts.append(context)\n",
        "            labels.append(label)\n",
        "    return targets, contexts, labels"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaXZP7gYUpzP"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k25Kg28XSaNt",
        "outputId": "ebf4b7b1-7a9e-4e2c-fc85-644a9179286a"
      },
      "source": [
        "# Sequences is a list of numpy arrays\n",
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences=sequences, \n",
        "    window_size=2, \n",
        "    num_ns=4, \n",
        "    vocab_size=vocab_size, \n",
        "    seed=SEED)\n",
        "print(len(targets), len(contexts), len(labels))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 32777/32777 [04:11<00:00, 130.52it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "65162 65162 65162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUq548DhUogm",
        "outputId": "f6261217-6f90-43fe-b75a-6de44cc04b14"
      },
      "source": [
        "BATCH_SIZE = 1000\n",
        "BUFFER_SIZE = 1000\n",
        "target_ragged = tf.ragged.constant(targets)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((target_ragged, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: (((1000, None), (1000, 5, 1)), (1000, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BFzqegHVvP2",
        "outputId": "a04912b3-8f9c-40c7-b82b-82ffc2e6d7f9"
      },
      "source": [
        "for (t,c),l in dataset.take(1):\n",
        "    print(t.shape)\n",
        "    print(c.shape)\n",
        "    print(l.shape)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, None)\n",
            "(1000, 5, 1)\n",
            "(1000, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ICw6RSjVwlF"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqICisaxWBGJ"
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dot, Embedding, Flatten"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVlHRzzKvGPi"
      },
      "source": [
        "num_ns = 4\n",
        "embedding_dim = 100\n",
        "target_embedding = Embedding(subword_vocab_size,\n",
        "                             embedding_dim,\n",
        "                             input_length=None,\n",
        "                             name=\"w2v_embedding\",)\n",
        "context_embedding = Embedding(vocab_size,\n",
        "                              embedding_dim,\n",
        "                              input_length=num_ns+1)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7IbZ8s8wwO0"
      },
      "source": [
        "dot = Dot(axes=(3,1))\n",
        "flatten = Flatten()"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-fXjdKUvwdS",
        "outputId": "d26e3663-fb91-4bc7-e3c4-fff10abb731f"
      },
      "source": [
        "for (t,c),l in dataset.take(1):\n",
        "    tar_em = tf.math.reduce_sum(target_embedding(t),axis=1)\n",
        "    con_em = context_embedding(c)\n",
        "    print(tar_em.shape)\n",
        "    print(con_em.shape)\n",
        "\n",
        "    d = dot([con_em, tar_em])\n",
        "    print(d.shape)\n",
        "\n",
        "    f = flatten(d)\n",
        "    print(f.shape)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 100)\n",
            "(1000, 5, 1, 100)\n",
            "(1000, 5, 1)\n",
            "(1000, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6emXRIfVv1x"
      },
      "source": [
        "class Word2Vec(Model):\n",
        "    def __init__(self, subword_vocab_size, vocab_size, embedding_dim):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.target_embedding = Embedding(subword_vocab_size+1, \n",
        "                                        embedding_dim,\n",
        "                                        input_length=None,\n",
        "                                        name=\"w2v_embedding\",)\n",
        "        self.context_embedding = Embedding(vocab_size+1, \n",
        "                                        embedding_dim, \n",
        "                                        input_length=num_ns+1)\n",
        "        self.dots = Dot(axes=(3,1))\n",
        "        self.flatten = Flatten()\n",
        "\n",
        "    def call(self, pair):\n",
        "        target, context = pair\n",
        "        we = tf.math.reduce_sum(self.target_embedding(target),axis=1)\n",
        "        ce = self.context_embedding(context)\n",
        "        dots = self.dots([ce, we])\n",
        "        return self.flatten(dots)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We45REeyWTPL"
      },
      "source": [
        "num_ns = 4\n",
        "embedding_dim = 128\n",
        "word2vec = Word2Vec(subword_vocab_size,vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 metrics=['accuracy'])"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJuRh2tjzohd"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "immtUofbWb8X",
        "outputId": "f4c98a85-a353-47fb-fd2e-6ba8a8f1e9f3"
      },
      "source": [
        "word2vec.fit(dataset, epochs=10, callbacks=[tensorboard_callback])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "65/65 [==============================] - 2s 21ms/step - loss: 1.5625 - accuracy: 0.4132\n",
            "Epoch 2/10\n",
            "65/65 [==============================] - 1s 19ms/step - loss: 0.7235 - accuracy: 0.8248\n",
            "Epoch 3/10\n",
            "65/65 [==============================] - 1s 20ms/step - loss: 0.5049 - accuracy: 0.8301\n",
            "Epoch 4/10\n",
            "65/65 [==============================] - 1s 19ms/step - loss: 0.4727 - accuracy: 0.8360\n",
            "Epoch 5/10\n",
            "65/65 [==============================] - 1s 19ms/step - loss: 0.4464 - accuracy: 0.8428\n",
            "Epoch 6/10\n",
            "65/65 [==============================] - 1s 19ms/step - loss: 0.4175 - accuracy: 0.8526\n",
            "Epoch 7/10\n",
            "65/65 [==============================] - 1s 19ms/step - loss: 0.3831 - accuracy: 0.8666\n",
            "Epoch 8/10\n",
            "65/65 [==============================] - 1s 19ms/step - loss: 0.3436 - accuracy: 0.8851\n",
            "Epoch 9/10\n",
            "65/65 [==============================] - 1s 19ms/step - loss: 0.2997 - accuracy: 0.9058\n",
            "Epoch 10/10\n",
            "65/65 [==============================] - 1s 19ms/step - loss: 0.2549 - accuracy: 0.9258\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f51e46fcbe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wemlndN_gaR7"
      },
      "source": [
        "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN-eVM3TgeNx"
      },
      "source": [
        "import io"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAbwd7Wkgazf"
      },
      "source": [
        "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if  index == 0: continue # skip 0, it's padding.\n",
        "  vec = weights[index] \n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "K365ask4gdO2",
        "outputId": "6db7f2ed-171f-4f86-edab-8886d8b7df8f"
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "  files.download('vectors.tsv')\n",
        "  files.download('metadata.tsv')\n",
        "except Exception as e:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6f08000c-eda1-483d-ab84-cad3a6a735da\", \"vectors.tsv\", 6035154)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6201a1b1-46ec-4af1-a8ac-721813dab4d6\", \"metadata.tsv\", 31290)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}