{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec-ablation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOi6DSG0iuqmgMvar/YbN1S"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9urcfcw42KNj"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook we will try to ablate on some of the discrepancies that we noticed with the [tensorflow official tutorial for Word2Vec](https://www.tensorflow.org/tutorials/text/word2vec).\n",
        "\n",
        "To be precise we are looking forward to ablate on two pointers:\n",
        "- Using the text vectorization layer\n",
        "- Customizing the negative pairs so that we do not have negative words from the window specified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_tJfAiQ2HF1",
        "outputId": "f2bd1380-0767-4514-e6c3-c7dd47012c64"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "SEED = 42 \n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyU_0-QXQRXW"
      },
      "source": [
        "# Skip-Grams with a single sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJM6syn344rR",
        "outputId": "0b2e0944-446e-40f8-c42e-7c40bca7810f"
      },
      "source": [
        "sentence = \"The wide road shimmered in the hot sun\"\n",
        "\n",
        "# tokenizer the sentence\n",
        "tokens = list(sentence.lower().split())\n",
        "print(f'Number of tokens: {len(tokens)}')\n",
        "\n",
        "# create word2index\n",
        "word_index = {}\n",
        "index = 1\n",
        "word_index['<pad>'] = 0 # add a padding token \n",
        "for token in tokens:\n",
        "  if token not in word_index: \n",
        "    word_index[token] = index\n",
        "    index += 1\n",
        "vocab_size = len(word_index)\n",
        "print(f'Vocab: {word_index}')\n",
        "\n",
        "inverse_vocab = {index: token for token, index in word_index.items()}\n",
        "print(f'Inverse Vocab: {inverse_vocab}')\n",
        "\n",
        "example_sequence = [word_index[word] for word in tokens]\n",
        "print(f'Tokenized sentence: {example_sequence}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of tokens: 8\n",
            "Vocab: {'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n",
            "Inverse Vocab: {0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n",
            "Tokenized sentence: [1, 2, 3, 4, 5, 1, 6, 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP75sSpj7zUw",
        "outputId": "8d1c6ba6-73bd-4634-8ede-a58946feefc9"
      },
      "source": [
        "# The positive skip grams\n",
        "window_size = 2\n",
        "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "      example_sequence, \n",
        "      vocabulary_size=vocab_size,\n",
        "      window_size=window_size,\n",
        "      negative_samples=0)\n",
        "print(len(positive_skip_grams))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vassMcf573II",
        "outputId": "d36ab2a2-3fdc-4db1-d9dc-3cbf6d75ad6d"
      },
      "source": [
        "print(f\"Sentence: {sentence}\")\n",
        "for target, context in positive_skip_grams[:5]:\n",
        "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: The wide road shimmered in the hot sun\n",
            "(2, 4): (wide, shimmered)\n",
            "(6, 7): (hot, sun)\n",
            "(1, 4): (the, shimmered)\n",
            "(1, 5): (the, in)\n",
            "(3, 2): (road, wide)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMicd7sVKbSW"
      },
      "source": [
        "import random"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEdc-13p8AnT",
        "outputId": "fc2de5e2-def6-4a8e-ddfb-a531901b665a"
      },
      "source": [
        "# Get target and context words for one positive skip-gram.\n",
        "target_word, context_word = positive_skip_grams[0]\n",
        "print(f\"({target_word}, {context_word}): ({inverse_vocab[target_word]}, {inverse_vocab[context_word]})\")\n",
        "# Set the number of negative samples per positive context. \n",
        "num_ns = 4\n",
        "\n",
        "list_of_words = list(range(100))\n",
        "\n",
        "context_words = [b if a == target_word else -1 for a,b in positive_skip_grams] + [target_word]\n",
        "context_words = list(filter(lambda x: x != -1, context_words))\n",
        "print(f'Context Words for `{target_word}`: {context_words}')\n",
        "\n",
        "negative_words = list(filter(lambda i: i not in context_words, list_of_words))\n",
        "\n",
        "negative_sampling_candidates = tf.constant(random.sample(negative_words, num_ns))\n",
        "\n",
        "print(negative_sampling_candidates)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 4): (wide, shimmered)\n",
            "Context Words for `2`: [4, 3, 1, 2]\n",
            "tf.Tensor([32 85 34 74], shape=(4,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SELYYmUt3GCw"
      },
      "source": [
        "# Data\n",
        "We will be working on the same data that the official guide uses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DJwhEkl3eCH"
      },
      "source": [
        "from tensorflow.keras.utils import get_file"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8j_en4D3FpA",
        "outputId": "8f9de2fe-dbd3-4a7a-bcaa-f6d8b2ce79ec"
      },
      "source": [
        "# Shakespear text file\n",
        "path_to_file = get_file(fname='warpeace_input.txt',\n",
        "                        origin='https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt')\n",
        "\n",
        "print(f'[INFO] Path to file: {path_to_file}')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt\n",
            "3260416/3258246 [==============================] - 1s 0us/step\n",
            "[INFO] Path to file: /root/.keras/datasets/warpeace_input.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5RmonRW364e"
      },
      "source": [
        "## Text\n",
        "Here in this snippet we will look into the text file. I would suggest people to take some time out and look into the data, even if it is just glancing it once. This step is not mandatory, but does build a mental map of what we are going to model up on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTfzNhM13DT2",
        "outputId": "5662e831-78db-4516-aca1-db040ef69c9c"
      },
      "source": [
        "# To vizualise the text data\n",
        "with open(path_to_file) as f:\n",
        "    lines = f.read().splitlines()\n",
        "for line in lines[:5]:\n",
        "    print(line)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ï»¿\"Well, Prince, so Genoa and Lucca are now just family estates of the\n",
            "Buonapartes. But I warn you, if you don't tell me that this means war,\n",
            "if you still try to defend the infamies and horrors perpetrated by that\n",
            "Antichrist--I really believe he is Antichrist--I will have nothing more\n",
            "to do with you and you are no longer my friend, no longer my 'faithful\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zypPcoTu4a3T",
        "outputId": "d2b7bae9-6295-492d-ac5a-2250c56ee98e"
      },
      "source": [
        "# Create a `tf.data` with all the non-negative sentences\n",
        "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
        "\n",
        "for text in text_ds.take(5):\n",
        "    print(text)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'\\xef\\xbb\\xbf\"Well, Prince, so Genoa and Lucca are now just family estates of the', shape=(), dtype=string)\n",
            "tf.Tensor(b\"Buonapartes. But I warn you, if you don't tell me that this means war,\", shape=(), dtype=string)\n",
            "tf.Tensor(b'if you still try to defend the infamies and horrors perpetrated by that', shape=(), dtype=string)\n",
            "tf.Tensor(b'Antichrist--I really believe he is Antichrist--I will have nothing more', shape=(), dtype=string)\n",
            "tf.Tensor(b\"to do with you and you are no longer my friend, no longer my 'faithful\", shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SA3sQo1QhO1"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import re\n",
        "import string"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wvmrzbG4zNZ"
      },
      "source": [
        "# We create a custom standardization function to lowercase the text and \n",
        "# remove punctuation.\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    return tf.strings.regex_replace(lowercase,\n",
        "                                    '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "# Define the vocabulary size and number of words in a sequence.\n",
        "vocab_size = 4096\n",
        "sequence_length = 20\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Set output_sequence_length length to pad all samples to same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "# build the vocab\n",
        "vectorize_layer.adapt(text_ds.batch(1024))"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO6QpTbTQcFs",
        "outputId": "5fc3ea78-c28b-4187-b229-25930c51d5f5"
      },
      "source": [
        "# Save the created vocabulary for reference.\n",
        "index_word = vectorize_layer.get_vocabulary()\n",
        "print(index_word[:20])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '[UNK]', 'the', 'and', 'to', 'of', 'a', 'he', 'in', 'his', 'that', 'was', 'with', 'had', 'it', 'her', 'not', 'him', 'at', 'i']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GGnYjuvRNEk"
      },
      "source": [
        "# Vectorize the data in text_ds.\n",
        "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afNzzNrrRVhK",
        "outputId": "c21a1609-6090-44d5-9e6d-2a4d22182563"
      },
      "source": [
        "for text in text_vector_ds.take(2):\n",
        "    print(text)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[   1   40   41    1    3    1   58   54  130  461 1479    5    2    0\n",
            "    0    0    0    0    0    0], shape=(20,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[  1  20  19   1  23  56  23 139 196  57  10  36 856 232   0   0   0   0\n",
            "   0   0], shape=(20,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0-VvNYlRX4J",
        "outputId": "91f48d9a-1ebf-4b0a-e3bb-78f7f3fa7efe"
      },
      "source": [
        "# sequences is a list of numpy arrays\n",
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(len(sequences))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJG_hecvRb7E",
        "outputId": "cf98477d-3ef1-4903-90c5-3de444fc8153"
      },
      "source": [
        "for seq in sequences[:5]:\n",
        "  print(f\"{seq} => {[index_word[i] for i in seq]}\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   1   40   41    1    3    1   58   54  130  461 1479    5    2    0\n",
            "    0    0    0    0    0    0] => ['[UNK]', 'prince', 'so', '[UNK]', 'and', '[UNK]', 'are', 'now', 'just', 'family', 'estates', 'of', 'the', '', '', '', '', '', '', '']\n",
            "[  1  20  19   1  23  56  23 139 196  57  10  36 856 232   0   0   0   0\n",
            "   0   0] => ['[UNK]', 'but', 'i', '[UNK]', 'you', 'if', 'you', 'dont', 'tell', 'me', 'that', 'this', 'means', 'war', '', '', '', '', '', '']\n",
            "[  56   23  104  852    4 2633    2    1    3    1    1   32   10    0\n",
            "    0    0    0    0    0    0] => ['if', 'you', 'still', 'try', 'to', 'defend', 'the', '[UNK]', 'and', '[UNK]', '[UNK]', 'by', 'that', '', '', '', '', '', '', '']\n",
            "[  1 313 502   7  26   1  64  39 161  65   0   0   0   0   0   0   0   0\n",
            "   0   0] => ['[UNK]', 'really', 'believe', 'he', 'is', '[UNK]', 'will', 'have', 'nothing', 'more', '', '', '', '', '', '', '', '', '', '']\n",
            "[   4   67   12   23    3   23   58   52  356   60  384   52  356   60\n",
            " 3591    0    0    0    0    0] => ['to', 'do', 'with', 'you', 'and', 'you', 'are', 'no', 'longer', 'my', 'friend', 'no', 'longer', 'my', 'faithful', '', '', '', '', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-kfMd1gSIgP"
      },
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "    # Elements of each training example are appended to these lists.\n",
        "    targets, contexts, labels = [], [], []\n",
        "    \n",
        "    # will be used to sample\n",
        "    list_of_words = list(range(vocab_size))\n",
        "    \n",
        "    # Build the sampling table for vocab_size tokens.\n",
        "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "    \n",
        "    # Iterate over all sequences (sentences) in dataset.\n",
        "    for sequence in tqdm(sequences):\n",
        "        # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "            sequence, \n",
        "            vocabulary_size=vocab_size,\n",
        "            sampling_table=sampling_table,\n",
        "            window_size=window_size,\n",
        "            negative_samples=0)\n",
        "        \n",
        "        # Iterate over each positive skip-gram pair to produce training examples \n",
        "        # with positive context word and negative samples.\n",
        "        for target_word, context_word in positive_skip_grams:\n",
        "            context_words = [context if target == target_word else -1 for target,context in positive_skip_grams] + [target_word]\n",
        "            context_words = list(filter(lambda x: x != -1, context_words))\n",
        "            \n",
        "            context_class = tf.expand_dims(\n",
        "                tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "            \n",
        "            negative_words = list(filter(lambda i: i not in context_words, list_of_words))\n",
        "            negative_sampling_candidates = tf.constant(random.sample(negative_words, num_ns), dtype=\"int64\")\n",
        "            \n",
        "            # Build context and label vectors (for one target word)\n",
        "            negative_sampling_candidates = tf.expand_dims(\n",
        "                negative_sampling_candidates, 1)\n",
        "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "            \n",
        "            # Append each element from the training example to global lists.\n",
        "            targets.append(target_word)\n",
        "            contexts.append(context)\n",
        "            labels.append(label)\n",
        "    return targets, contexts, labels"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaXZP7gYUpzP"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k25Kg28XSaNt",
        "outputId": "6d64b71e-9948-4b5f-b8ed-93619dbe06f6"
      },
      "source": [
        "# Sequences is a list of numpy arrays\n",
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences=sequences, \n",
        "    window_size=2, \n",
        "    num_ns=4, \n",
        "    vocab_size=vocab_size, \n",
        "    seed=SEED)\n",
        "print(len(targets), len(contexts), len(labels))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|ââââââââââ| 50506/50506 [14:20<00:00, 58.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "202204 202204 202204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUq548DhUogm",
        "outputId": "d20debec-3dec-4e51-d4df-d013b28586bf"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ICw6RSjVwlF"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqICisaxWBGJ"
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dot, Embedding, Flatten"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6emXRIfVv1x"
      },
      "source": [
        "class Word2Vec(Model):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.target_embedding = Embedding(vocab_size, \n",
        "                                        embedding_dim,\n",
        "                                        input_length=1,\n",
        "                                        name=\"w2v_embedding\", )\n",
        "        self.context_embedding = Embedding(vocab_size, \n",
        "                                        embedding_dim, \n",
        "                                        input_length=num_ns+1)\n",
        "        self.dots = Dot(axes=(3,2))\n",
        "        self.flatten = Flatten()\n",
        "\n",
        "    def call(self, pair):\n",
        "        target, context = pair\n",
        "        we = self.target_embedding(target)\n",
        "        ce = self.context_embedding(context)\n",
        "        dots = self.dots([ce, we])\n",
        "        return self.flatten(dots)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga2JG7tXVn7R"
      },
      "source": [
        "def custom_loss(x_logit, y_true):\n",
        "    return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We45REeyWTPL"
      },
      "source": [
        "embedding_dim = 128\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 metrics=['accuracy'])"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7Cg0_2sWZqb"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "immtUofbWb8X",
        "outputId": "a60e3201-4e35-483e-990e-233113e638cb"
      },
      "source": [
        "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "197/197 [==============================] - 4s 18ms/step - loss: 1.5692 - accuracy: 0.4115\n",
            "Epoch 2/20\n",
            "197/197 [==============================] - 4s 18ms/step - loss: 0.7281 - accuracy: 0.8233\n",
            "Epoch 3/20\n",
            "197/197 [==============================] - 3s 17ms/step - loss: 0.5140 - accuracy: 0.8245\n",
            "Epoch 4/20\n",
            "197/197 [==============================] - 4s 18ms/step - loss: 0.4894 - accuracy: 0.8274\n",
            "Epoch 5/20\n",
            "197/197 [==============================] - 4s 18ms/step - loss: 0.4745 - accuracy: 0.8315\n",
            "Epoch 6/20\n",
            "197/197 [==============================] - 4s 18ms/step - loss: 0.4561 - accuracy: 0.8377\n",
            "Epoch 7/20\n",
            "197/197 [==============================] - 4s 18ms/step - loss: 0.4349 - accuracy: 0.8459\n",
            "Epoch 8/20\n",
            "197/197 [==============================] - 4s 18ms/step - loss: 0.4079 - accuracy: 0.8567\n",
            "Epoch 9/20\n",
            "197/197 [==============================] - 4s 18ms/step - loss: 0.3767 - accuracy: 0.8699\n",
            "Epoch 10/20\n",
            "197/197 [==============================] - 4s 18ms/step - loss: 0.3414 - accuracy: 0.8854\n",
            "Epoch 11/20\n",
            "197/197 [==============================] - 4s 18ms/step - loss: 0.3048 - accuracy: 0.9026\n",
            "Epoch 12/20\n",
            "197/197 [==============================] - 4s 18ms/step - loss: 0.2679 - accuracy: 0.9203\n",
            "Epoch 13/20\n",
            "197/197 [==============================] - 4s 19ms/step - loss: 0.2306 - accuracy: 0.9374\n",
            "Epoch 14/20\n",
            "197/197 [==============================] - 4s 18ms/step - loss: 0.1960 - accuracy: 0.9530\n",
            "Epoch 15/20\n",
            "197/197 [==============================] - 4s 19ms/step - loss: 0.1649 - accuracy: 0.9662\n",
            "Epoch 16/20\n",
            "197/197 [==============================] - 4s 18ms/step - loss: 0.1379 - accuracy: 0.9760\n",
            "Epoch 17/20\n",
            "197/197 [==============================] - 4s 18ms/step - loss: 0.1145 - accuracy: 0.9834\n",
            "Epoch 18/20\n",
            "197/197 [==============================] - 4s 18ms/step - loss: 0.0948 - accuracy: 0.9885\n",
            "Epoch 19/20\n",
            "197/197 [==============================] - 4s 18ms/step - loss: 0.0790 - accuracy: 0.9919\n",
            "Epoch 20/20\n",
            "197/197 [==============================] - 4s 18ms/step - loss: 0.0657 - accuracy: 0.9942\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa561ee8518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wemlndN_gaR7"
      },
      "source": [
        "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN-eVM3TgeNx"
      },
      "source": [
        "import io"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAbwd7Wkgazf"
      },
      "source": [
        "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if  index == 0: continue # skip 0, it's padding.\n",
        "  vec = weights[index] \n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "K365ask4gdO2",
        "outputId": "6db7f2ed-171f-4f86-edab-8886d8b7df8f"
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "  files.download('vectors.tsv')\n",
        "  files.download('metadata.tsv')\n",
        "except Exception as e:\n",
        "  pass"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6f08000c-eda1-483d-ab84-cad3a6a735da\", \"vectors.tsv\", 6035154)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6201a1b1-46ec-4af1-a8ac-721813dab4d6\", \"metadata.tsv\", 31290)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}