{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Glove.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP1jZdVjCMaA66NGzYYCXrc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cr0wley-zz/Embeddings/blob/main/Glove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DVI-uUrJzrj"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei4bwYUFJ7v6"
      },
      "source": [
        "data = \"\"\"\n",
        "My name is Devjyoti. Your name is Aritra. Aritra loves coding. Devjyoti Loves Sleeping. Aritra loves cricket. Devjyoti loves watching movies. Aritra likes \n",
        "western movies. Devjyoti likes eastern movies. \n",
        "\"\"\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_qad3FyKS4Z"
      },
      "source": [
        "def word_processor(data):\n",
        "  # Converts the data into tokens\n",
        "  tokenized_text = tf.keras.preprocessing.text.text_to_word_sequence(data)\n",
        "  tokenized_text_size = len(tokenized_text)\n",
        "  # Creates a vocab of unique words\n",
        "  vocab = sorted(set(tokenized_text))\n",
        "  vocab_size = len(vocab)\n",
        "  # Map the vocab words to individual indices\n",
        "  vocab_to_ix = {c:ix for ix,c in enumerate(vocab)}\n",
        "  # Map the indices to the words in vocab\n",
        "  ix_to_vocab = np.array(vocab)\n",
        "  # Convert the data into numbers\n",
        "  text_as_int = np.array([vocab_to_ix[c] for c in tokenized_text])\n",
        "  return vocab ,vocab_size, vocab_to_ix, ix_to_vocab, text_as_int"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoBdP8ruOIv3",
        "outputId": "1fe86a9a-1586-466b-e0f1-bba6856b474b"
      },
      "source": [
        "word_processor(data)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['aritra',\n",
              "  'coding',\n",
              "  'cricket',\n",
              "  'devjyoti',\n",
              "  'eastern',\n",
              "  'is',\n",
              "  'likes',\n",
              "  'loves',\n",
              "  'movies',\n",
              "  'my',\n",
              "  'name',\n",
              "  'sleeping',\n",
              "  'watching',\n",
              "  'western',\n",
              "  'your'],\n",
              " 15,\n",
              " {'aritra': 0,\n",
              "  'coding': 1,\n",
              "  'cricket': 2,\n",
              "  'devjyoti': 3,\n",
              "  'eastern': 4,\n",
              "  'is': 5,\n",
              "  'likes': 6,\n",
              "  'loves': 7,\n",
              "  'movies': 8,\n",
              "  'my': 9,\n",
              "  'name': 10,\n",
              "  'sleeping': 11,\n",
              "  'watching': 12,\n",
              "  'western': 13,\n",
              "  'your': 14},\n",
              " array(['aritra', 'coding', 'cricket', 'devjyoti', 'eastern', 'is',\n",
              "        'likes', 'loves', 'movies', 'my', 'name', 'sleeping', 'watching',\n",
              "        'western', 'your'], dtype='<U8'),\n",
              " array([ 9, 10,  5,  3, 14, 10,  5,  0,  0,  7,  1,  3,  7, 11,  0,  7,  2,\n",
              "         3,  7, 12,  8,  0,  6, 13,  8,  3,  6,  4,  8]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9Z_lJtJKa9R",
        "outputId": "48aa7b1a-7f77-4e29-8a0c-c650ad173d5a"
      },
      "source": [
        "co_occurance_matrix = np.zeros((vocab_size, vocab_size))\n",
        "co_occurance_matrix.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGkunm8HLORS"
      },
      "source": [
        "def compute_co_occurrence_matrix(corpus, window_size=1):\n",
        "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
        "\n",
        "      Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
        "            number of co-occurring words.\n",
        "            \n",
        "            For example, if we take the document \"<START> All that glitters is not gold <END>\" with window size of 4,\n",
        "            \"All\" will co-occur with \"<START>\", \"that\", \"glitters\", \"is\", and \"not\".\n",
        "\n",
        "      Params:\n",
        "          corpus (list of list of strings): corpus of documents\n",
        "          window_size (int): size of context window\n",
        "      Return:\n",
        "          M (a symmetric numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): \n",
        "              Co-occurence matrix of word counts. \n",
        "              The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
        "          word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
        "    \"\"\"\n",
        "    vocab ,vocab_size, vocab_to_ix, ix_to_vocab, text_as_int  = word_processor(corpus)\n",
        "    \n",
        "    \n",
        "\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    M = np.zeros((vocab_size, vocab_size))\n",
        "\n",
        "    doc_len =  len(text_as_int)\n",
        "    for current_idx in range(doc_len):\n",
        "      left_boudary = max(current_idx-window_size, 0)\n",
        "      right_boundary = min(current_idx+window_size+1, doc_len)\n",
        "      outside_words = np.append(text_as_int[left_boudary:current_idx],text_as_int[current_idx+1:right_boundary])\n",
        "      center_word = text_as_int[current_idx]\n",
        "      \n",
        "      \n",
        "      for outside_word in outside_words:\n",
        "          M[center_word, outside_word] += 1\n",
        "      \n",
        "      current_idx += 1\n",
        "    # ------------------\n",
        "\n",
        "    return M, vocab_to_ix, vocab_size\n",
        "\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPPlf6ikLznQ"
      },
      "source": [
        "co_occurance_matrix ,vocab_to_ix, vocab_size  = compute_co_occurrence_matrix(data, 1)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVh3NZhPQoUJ",
        "outputId": "70a24d57-2564-44c9-c447-80c5789ac73c"
      },
      "source": [
        "co_occurance_matrix"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2., 0., 0., 0., 0., 1., 1., 2., 1., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 1., 0., 0., 1., 1., 2., 1., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [2., 1., 1., 2., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.],\n",
              "       [1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 2., 0., 0., 0., 1., 0., 0., 0., 0., 1.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ4-hNJ7SgRw",
        "outputId": "476f0e93-8d24-4a88-dcd4-2493e063ccd7"
      },
      "source": [
        "vocab_to_ix"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'aritra': 0,\n",
              " 'coding': 1,\n",
              " 'cricket': 2,\n",
              " 'devjyoti': 3,\n",
              " 'eastern': 4,\n",
              " 'is': 5,\n",
              " 'likes': 6,\n",
              " 'loves': 7,\n",
              " 'movies': 8,\n",
              " 'my': 9,\n",
              " 'name': 10,\n",
              " 'sleeping': 11,\n",
              " 'watching': 12,\n",
              " 'western': 13,\n",
              " 'your': 14}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFj83GOASrS3"
      },
      "source": [
        "probability_matrix = co_occurance_matrix/co_occurance_matrix.sum(axis = 0)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9po5TdKTi5Y",
        "outputId": "002babbf-1dce-4804-dea3-56035f8e396f"
      },
      "source": [
        "co_occurance_matrix.sum(axis = 0)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8., 2., 2., 8., 2., 4., 4., 8., 5., 1., 4., 2., 2., 2., 2.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLHir6H5Tkew",
        "outputId": "b123139d-305a-4f39-f538-f67a9f78d261"
      },
      "source": [
        "probability_matrix"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.25 , 0.   , 0.   , 0.   , 0.   , 0.25 , 0.25 , 0.25 , 0.2  ,\n",
              "        0.   , 0.   , 0.5  , 0.   , 0.   , 0.   ],\n",
              "       [0.   , 0.   , 0.   , 0.125, 0.   , 0.   , 0.   , 0.125, 0.   ,\n",
              "        0.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n",
              "       [0.   , 0.   , 0.   , 0.125, 0.   , 0.   , 0.   , 0.125, 0.   ,\n",
              "        0.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n",
              "       [0.   , 0.5  , 0.5  , 0.   , 0.   , 0.25 , 0.25 , 0.25 , 0.2  ,\n",
              "        0.   , 0.   , 0.   , 0.   , 0.   , 0.5  ],\n",
              "       [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.25 , 0.   , 0.2  ,\n",
              "        0.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n",
              "       [0.125, 0.   , 0.   , 0.125, 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
              "        0.   , 0.5  , 0.   , 0.   , 0.   , 0.   ],\n",
              "       [0.125, 0.   , 0.   , 0.125, 0.5  , 0.   , 0.   , 0.   , 0.   ,\n",
              "        0.   , 0.   , 0.   , 0.   , 0.5  , 0.   ],\n",
              "       [0.25 , 0.5  , 0.5  , 0.25 , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
              "        0.   , 0.   , 0.5  , 0.5  , 0.   , 0.   ],\n",
              "       [0.125, 0.   , 0.   , 0.125, 0.5  , 0.   , 0.   , 0.   , 0.   ,\n",
              "        0.   , 0.   , 0.   , 0.5  , 0.5  , 0.   ],\n",
              "       [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
              "        0.   , 0.25 , 0.   , 0.   , 0.   , 0.   ],\n",
              "       [0.   , 0.   , 0.   , 0.   , 0.   , 0.5  , 0.   , 0.   , 0.   ,\n",
              "        1.   , 0.   , 0.   , 0.   , 0.   , 0.5  ],\n",
              "       [0.125, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.125, 0.   ,\n",
              "        0.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n",
              "       [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.125, 0.2  ,\n",
              "        0.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n",
              "       [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.25 , 0.   , 0.2  ,\n",
              "        0.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n",
              "       [0.   , 0.   , 0.   , 0.125, 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
              "        0.   , 0.25 , 0.   , 0.   , 0.   , 0.   ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEl1dLm5Trvw"
      },
      "source": [
        "EMBEDDING_SIZE = 2\n",
        "context_vector =  tf.Variable(np.random.rand(vocab_size, EMBEDDING_SIZE))\n",
        "center_vector = tf.Variable(np.random.rand(vocab_size, EMBEDDING_SIZE))\n",
        "bias_center = tf.Variable(np.random.rand(vocab_size, 1))\n",
        "bias_context = tf.Variable(np.random.rand(vocab_size, 1))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzbMWkvxYO6N"
      },
      "source": [
        "def weighted_func(x):\n",
        "  return np.power((x/100), (3/4)) if x< 100 else 1"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS3bhvzFUVs4"
      },
      "source": [
        "def train_step(indices, loss_list):\n",
        "  \"\"\"The training step\n",
        "\n",
        "  Arguments:\n",
        "    indices (list): The indices of the vocab in the window\n",
        "  \"\"\"\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "     \n",
        "   \n",
        "    \n",
        "  loss_list.append(np.array(log_loss))\n",
        "  grad = tape.gradient(log_loss, [context_vector, center_vector])\n",
        "  opt.apply_gradients(zip(grad, [context_vector, center_vector]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHwtDQTFXLbx"
      },
      "source": [
        "loss_list = []\n",
        "for iter in tqdm(range(iterations)):\n",
        "  # Creation of window\n",
        "  for i in (vocab):\n",
        "    for j in rang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWun9pSeXLeJ",
        "outputId": "51a99218-4575-4c2a-b1a6-0943dcbf2d0b"
      },
      "source": [
        "text_as_int"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9, 10,  5,  3, 14, 10,  5,  0,  0,  7,  1,  3,  7, 11,  0,  7,  2,\n",
              "        3,  7, 12,  8,  0,  6, 13,  8,  3,  6,  4,  8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac6FsXzfXLgd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}