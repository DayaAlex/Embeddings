{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Words2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOxEwAe+Dch+MRv1Cuc+3hy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcidF21Au81T"
      },
      "source": [
        "# Likelihood:\n",
        "For each position $t=1,2..T$ we predict the context words in a window of fixed size $m$, given the center word $w_{t}$.\n",
        "$$\n",
        "L( \\theta ) =\\prod ^{T}_{t=1}\\prod _{-m\\leqslant j\\leqslant m;\\ j\\neq 0} P( w_{t+j} |w_{t} ;\\theta )\n",
        "$$\n",
        "\n",
        "The symbols and their meaning:\n",
        "1. $\\theta$: The parameter that is learned. Here refers to the vector embeddings of each word.\n",
        "2. $t$: The variable that represents each word index in the corpus of text. There are $T$ number of words in the corpus.\n",
        "3. $m$: The size of the window of context\n",
        "4. $j$: The variable that represents the context words index.\n",
        "\n",
        "We can say that likelihood is how good our model has done. How likely is our probabilistic model going to predict the right context words given the center word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f9S8SJayZOA"
      },
      "source": [
        "# Objective Function\n",
        "The objective function is taken to be the average negative log likelihood.\n",
        "\n",
        "Average: The average gives us the mean error. Scale of things are independent of the size of the corpus.\n",
        "\n",
        "Negative: The optimizer should decrease the error\n",
        "\n",
        "Log: Products are transformed into summation\n",
        "\n",
        "$$\n",
        "J( \\theta ) =-\\frac{1}{T}\\log L( \\theta )\\\\\n",
        "\\Longrightarrow J( \\theta ) =-\\frac{1}{T}\\sum ^{T}_{t=1}\\sum _{-m\\leqslant j\\leqslant m;\\ j\\neq 0}\\log P( w_{t+j} |w_{t} ;\\theta )\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeckyklxKpIJ"
      },
      "source": [
        "# Probability\n",
        "Everything is well and fine. The probability model and the objective function is intuitive enough. The word vectors would indeed make a lot of sense if those vectors could be used to predict the context vectors. The only problem that we see right now is with the probability equation. What could we probably use to depict the probability of a context word given a center word?\n",
        "\n",
        "What we consider having is two vector representation for each word in the vocabulary. Let us consider a word $w$ then the two vectors would be:\n",
        "1. $v_w$ - When the word is the **center** word\n",
        "2. $u_w$ - When the words is the **context** word\n",
        "\n",
        "$$\n",
        "\\boxed{P( w_{o} |w_{c}) =\\frac{\\exp u_{o}^{T} v_{c}}{\\sum ^{W}_{i=1}\\exp u_{o}^{T} v_{c}}}\n",
        "$$\n",
        "\n",
        "This looks like the **softmax function** doesn't it? Let us break this equation down.\n",
        "\n",
        "In the numerator we have the $\\exp u_{o}^{T} v_{c}$ term. This is the dot product between the center and the context word. This signifies how close the two words are. Exponentiating the dot product has a nice effect to it. It not only increases the big numbers but also diminishes the small number. Hence the numerator talks about how close (similar) the center and context words are.\n",
        "\n",
        "In the denominator we have a normalization term $\\sum ^{V}_{i=1}\\exp u_{i}^{T} v_{c}$. This normalizes the numerator and provides us with a percentage similarity.\n",
        "\n",
        "The formula diectly translates to the probability of the context words with the center word in question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiA4wH8mS9nm"
      },
      "source": [
        "# Optimization\n",
        "We need to notice that $\\theta$ here is the parameter of the model. The parameters are the vector representation of the words in the vocabulary. We consider a vocabulary of size $V$. The vectors are $d$ dimensional. We also need to understand that in our case we are considering two vector representations for each word ($u$ and $v$). This means that the parameter vector is going to be of the size of $2dV$.\n",
        "$$\n",
        "\\theta =\\begin{bmatrix}\n",
        "u_{the}\\\\\n",
        "u_{quick}\\\\\n",
        ".\\\\\n",
        ".\\\\\n",
        "u_{end}\\\\\n",
        "v_{the}\\\\\n",
        "v_{quick}\\\\\n",
        ".\\\\\n",
        ".\\\\\n",
        "v_{end}\n",
        "\\end{bmatrix} \\in \\Re ^{2dV}\n",
        "$$\n",
        "\n",
        "With this vector in place we would like to optimize each and every one of the vectors to descend down the **loss landscape** and model the meaning of each word. In this process, we would have to compute $\\nabla\\theta$. This directly means that we need to compute the gradient for each and every term in the vector $\\theta$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2Wc1bnZpu4t"
      },
      "source": [
        "# Loss landscape\n",
        "\n",
        "Here I will try to derive the gradients of the loss with respect to the parameters of the model. The parameters of the model includes $u_{w}$ and $v_{w}$ for each $w$ in the vocabulary. We initialise the model with random $u_{w}$ and $v_{w}$ and then change their values in accordance to the gradient of the objective function $J(\\theta)$.\n",
        "\n",
        "# $\\frac{\\partial J(\\theta)}{\\partial v_{t}}$\n",
        "\n",
        "In this section we will look into the derivation of the gradient of the objective function with respect to the vector representation of the center word.\n",
        "$$\n",
        "\\frac{\\partial \\log P( u_{o} |v_{c})}{\\partial v_{c}} =\\frac{\\partial }{\\partial v_{c}}\\log\\frac{\\exp u_{o} v_{c}}{\\sum ^{V}_{i=1}\\exp u_{i} v_{c}}\\\\\n",
        "\\Longrightarrow \\frac{\\partial \\log P( u_{o} |v_{c})}{\\partial v_{c}} =\\frac{\\partial }{\\partial v_{c}}\\log\\exp u_{o} v_{c} -\\frac{\\partial }{\\partial v_{c}}\\log\\sum ^{V}_{i=1}\\exp u_{i} v_{c}\\\\\n",
        "\\Longrightarrow \\frac{\\partial \\log P( u_{o} |v_{c})}{\\partial v_{c}} =\\frac{\\partial }{\\partial v_{c}} u_{o} v_{c} -\\frac{\\partial }{\\partial \\sum ^{V}_{i=1}\\exp u_{i} v_{c}}\\log\\sum ^{V}_{i=1}\\exp u_{i} v_{c} .\\frac{\\partial }{\\partial v_{c}}\\sum ^{V}_{x=1}\\exp u_{x} v_{c}\\\\\n",
        "\\Longrightarrow \\frac{\\partial \\log P( u_{o} |v_{c})}{\\partial v_{c}} =u_{o} -\\frac{1}{\\sum ^{V}_{i=1}\\exp u_{i} v_{c}} .\\sum ^{V}_{x=1}(\\exp u_{x} v_{c}) u_{x}\\\\\n",
        "\\Longrightarrow \\frac{\\partial \\log P( u_{o} |v_{c})}{\\partial v_{c}} =u_{o} -\\frac{\\sum ^{V}_{x=1}(\\exp u_{x} v_{c}) u_{x}}{\\sum ^{V}_{i=1}\\exp u_{i} v_{c}}\\\\\n",
        "\\Longrightarrow \\frac{\\partial \\log P( u_{o} |v_{c})}{\\partial v_{c}} =u_{o} -\\sum ^{V}_{x=1}\\frac{(\\exp u_{x} v_{c})}{\\sum ^{V}_{i=1}\\exp u_{i} v_{c}} u_{x}\\\\\n",
        "\\Longrightarrow \\frac{\\partial \\log P( u_{o} |v_{c})}{\\partial v_{c}} =u_{o} -\\sum ^{V}_{x=1} P( u_{x} |v_{c}) u_{x}\\\\\n",
        "\\boxed{\\frac{\\partial J( \\theta )}{\\partial v_{c}} =-\\frac{1}{T}\\sum ^{T}_{t-1}\\sum _{-m\\leqslant j\\leqslant m;\\ j\\neq 0}\\left[ u_{t+j} -\\sum ^{V}_{x=1} P( u_{x} |v_{t}) u_{x}\\right]}\n",
        "$$\n",
        "\n",
        "The last equation kind of gives us an intuitive model of the gradient (slope). We are subtracting the expected context word vector ($\\sum ^{V}_{x=1} P( u_{x} |v_{c}) u_{x}$) from the observed context vector ($u_o$).\n",
        "# $\\frac{\\partial P( J(\\theta)}{\\partial u_{o}}$\n",
        "\n",
        "In this section we will look into the derivation of the gradient of the objective function with respect to the vector representation of the context word.\n",
        "$$\n",
        "\\frac{\\partial \\log P( u_{o} |v_{c})}{\\partial u_{o}} =\\frac{\\partial }{\\partial u_{o}}\\log\\frac{\\exp u_{o} v_{c}}{\\sum ^{V}_{i=1}\\exp u_{i} v_{c}}\\\\\n",
        "\\Longrightarrow \\frac{\\partial \\log P( u_{o} |v_{c})}{\\partial u_{o}} =\\frac{\\partial }{\\partial u_{o}}\\log\\exp u_{o} v_{c} -\\frac{\\partial }{\\partial u_{o}}\\log\\sum ^{V}_{i=1}\\exp u_{i} v_{c}\\\\\n",
        "\\Longrightarrow \\frac{\\partial \\log P( u_{o} |v_{c})}{\\partial u_{o}} =\\frac{\\partial }{\\partial u_{o}} u_{o} v_{c} -\\frac{\\partial }{\\partial \\sum ^{V}_{i=1}\\exp u_{i} v_{c}}\\log\\sum ^{V}_{i=1}\\exp u_{i} v_{c} .\\frac{\\partial }{\\partial u_{o}}\\sum ^{V}_{x=1}\\exp u_{x} v_{c}\\\\\n",
        "\\Longrightarrow \\frac{\\partial \\log P( u_{o} |v_{c})}{\\partial u_{o}} =v_{c} -\\frac{1}{\\sum ^{V}_{i=1}\\exp u_{i} v_{c}} .v_{c}\\\\\n",
        "\\boxed{\\frac{\\partial J( \\theta )}{\\partial u_{t+j}} =-\\frac{1}{T}\\sum ^{T}_{t-1}\\sum _{-m\\leqslant j\\leqslant m;\\ j\\neq 0}\\left[ u_{t+j} -\\sum ^{V}_{x=1} P( u_{x} |v_{t}) u_{x}\\right]}\n",
        "$$"
      ]
    }
  ]
}